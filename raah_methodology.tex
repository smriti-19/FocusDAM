\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{enumitem}

\title{Region-Aware Dense Captioning via RAAH: \\
Attribute-Driven Caption Enhancement}
\author{}
\date{}

\begin{document}

\maketitle

\section{Methodology}

\subsection{Problem Formulation}

Given an image $\mathcal{I} \in \mathbb{R}^{H \times W \times 3}$ and a target region $\mathcal{R} \subset \mathcal{I}$ specified by a bounding box or segmentation mask, our objective is to generate natural language descriptions that accurately capture fine-grained visual attributes (color, material, shape, texture) of objects within $\mathcal{R}$. While standard vision-language models like DAM produce coherent captions, they often miss or hallucinate specific visual attributes, particularly for small or occluded objects. We propose \textbf{RAAH (Region-Aware Attribute Head)}, a lightweight attribute classifier trained on DAM's hidden states to extract and inject precise visual attributes into generated captions.

\subsection{Baseline Method: DAM-3B}

\subsubsection{Architecture Overview}

The Describe Anything Model (DAM-3B) serves as our baseline captioning system, employing a modular vision-language architecture with approximately 3 billion parameters. DAM processes images through three main components:

\paragraph{Vision Encoder.} DAM-3B utilizes SigLIP (Sigmoid Loss for Language-Image Pre-training) as its vision backbone $\mathcal{E}_v$. For a given image $\mathcal{I}$, the vision encoder produces a spatial grid of visual tokens:

\begin{equation}
\mathbf{V} = \mathcal{E}_v(\mathcal{I}) = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_N\} \in \mathbb{R}^{N \times d_v}
\end{equation}

where $N = h_p \times w_p$ is the number of spatial patches (typically $14 \times 14 = 196$ for $224 \times 224$ input resolution), and $d_v = 1152$ is the vision encoder's hidden dimension for SigLIP-SO400M.

\paragraph{Context-Aware Multi-Scale Encoding.} To provide region-specific information alongside global context, DAM employs a dual-view strategy. The input consists of concatenated image representations:

\begin{equation}
\mathcal{I}_{\text{input}} = [\mathcal{I}_{\text{full}} \mid \mathcal{I}_{\text{crop}}] \in \mathbb{R}^{H \times W \times 8}
\end{equation}

where $\mathcal{I}_{\text{full}}$ is the complete image with an alpha channel mask overlay indicating the region, and $\mathcal{I}_{\text{crop}}$ is a focused crop centered on the target region $\mathcal{R}$. The vision encoder processes both views:

\begin{align}
\mathbf{V}_{\text{full}} &= \mathcal{E}_v(\mathcal{I}_{\text{full}}) \in \mathbb{R}^{N \times d_v} \\
\mathbf{V}_{\text{crop}} &= \mathcal{E}_v(\mathcal{I}_{\text{crop}}) \in \mathbb{R}^{N \times d_v}
\end{align}

A context provider module $\mathcal{C}$ fuses these representations via cross-attention, where crop features query the full-image context:

\begin{align}
\mathbf{Q} &= \mathbf{V}_{\text{crop}} \mathbf{W}_Q, \quad \mathbf{K} = \mathbf{V}_{\text{full}} \mathbf{W}_K, \quad \mathbf{V}_c = \mathbf{V}_{\text{full}} \mathbf{W}_V \\
\mathbf{A} &= \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right) \in \mathbb{R}^{N \times N} \\
\mathbf{V}_{\text{fused}} &= \mathbf{A} \mathbf{V}_c \in \mathbb{R}^{N \times d_v}
\end{align}

where $d_k = d_v / H_{\text{heads}}$ and $H_{\text{heads}}$ is the number of attention heads.

\paragraph{Multimodal Projector.} A multi-layer perceptron (MLP) $\mathcal{P}$ projects visual features into the language model's embedding space. For a 2-layer MLP:

\begin{equation}
\mathcal{P}(\mathbf{V}_{\text{fused}}) = \mathbf{W}_2 \cdot \text{GELU}(\mathbf{W}_1 \mathbf{V}_{\text{fused}} + \mathbf{b}_1) + \mathbf{b}_2
\end{equation}

where $\mathbf{W}_1 \in \mathbb{R}^{d_h \times d_v}$, $\mathbf{W}_2 \in \mathbb{R}^{d_h \times d_h}$, and $d_h = 2560$ is the language model's hidden dimension (for LLaMA-2-7B backbone). The output is a sequence of visual embeddings:

\begin{equation}
\mathbf{X}_v = \mathcal{P}(\mathbf{V}_{\text{fused}}) = \{\mathbf{x}_1^v, \ldots, \mathbf{x}_N^v\} \in \mathbb{R}^{N \times d_h}
\end{equation}

\paragraph{Language Model Decoder.} DAM employs a LLaMA-based autoregressive transformer decoder with $L = 32$ layers. Given a text prompt encoded as token IDs $\mathbf{t} = (t_1, \ldots, t_S)$ with a special image token $\langle \text{IMG} \rangle$ at position $p_{\text{img}}$, the model constructs a hybrid input sequence:

\begin{equation}
\mathbf{X} = [\mathbf{E}(t_1), \ldots, \mathbf{E}(t_{p_{\text{img}}-1}), \mathbf{X}_v, \mathbf{E}(t_{p_{\text{img}}+1}), \ldots, \mathbf{E}(t_S)] \in \mathbb{R}^{L_{\text{seq}} \times d_h}
\end{equation}

where $\mathbf{E}: \mathcal{V} \rightarrow \mathbb{R}^{d_h}$ is the token embedding function, $\mathcal{V}$ is the vocabulary, and $L_{\text{seq}} = S - 1 + N$.

Each transformer layer $\ell$ applies multi-head self-attention followed by a feed-forward network:

\begin{align}
\mathbf{H}_\ell^{(0)} &= \text{LayerNorm}(\mathbf{H}_{\ell-1}) \\
\mathbf{H}_\ell^{(1)} &= \mathbf{H}_{\ell-1} + \text{MHA}_\ell(\mathbf{H}_\ell^{(0)}) \\
\mathbf{H}_\ell^{(2)} &= \text{LayerNorm}(\mathbf{H}_\ell^{(1)}) \\
\mathbf{H}_\ell &= \mathbf{H}_\ell^{(1)} + \text{FFN}_\ell(\mathbf{H}_\ell^{(2)})
\end{align}

where the feed-forward network uses SwiGLU activation:

\begin{equation}
\text{FFN}_\ell(\mathbf{x}) = \mathbf{W}_{\text{down}} \left( \text{SiLU}(\mathbf{W}_{\text{gate}} \mathbf{x}) \odot (\mathbf{W}_{\text{up}} \mathbf{x}) \right)
\end{equation}

with $\mathbf{W}_{\text{gate}}, \mathbf{W}_{\text{up}} \in \mathbb{R}^{d_{\text{ffn}} \times d_h}$, $\mathbf{W}_{\text{down}} \in \mathbb{R}^{d_h \times d_{\text{ffn}}}$, and $d_{\text{ffn}} = 4d_h = 10240$.

\paragraph{Autoregressive Generation.} The final layer's hidden states are projected to vocabulary logits:

\begin{equation}
\mathbf{z}_t = \mathbf{W}_{\text{lm}} \, \text{LayerNorm}(\mathbf{h}_{L, t}) \in \mathbb{R}^{|\mathcal{V}|}
\end{equation}

where $\mathbf{W}_{\text{lm}} \in \mathbb{R}^{|\mathcal{V}| \times d_h}$ is the language modeling head. Token generation follows:

\begin{equation}
p(y_t \mid y_{<t}, \mathcal{I}) = \text{softmax}(\mathbf{z}_t / \tau), \quad y_t \sim p(\cdot \mid y_{<t}, \mathcal{I})
\end{equation}

where $\tau$ is the sampling temperature.

\subsection{Proposed Method: RAAH}

While DAM generates fluent captions, it often lacks precise visual attributes or hallucinates details not present in the target region. RAAH addresses this by training a lightweight attribute classifier on DAM's decoder hidden states, enabling attribute extraction and caption augmentation without modifying DAM's weights.

\subsubsection{Motivation and Design Principles}

RAAH is guided by three key principles:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Training-free for DAM:} RAAH operates as a post-hoc module trained on frozen DAM hidden states, requiring no fine-tuning of the base captioning model.
    \item \textbf{Fine-grained attribute recovery:} Focus on recovering specific visual attributes (color, material, shape, texture) that are often under-represented in generic captions.
    \item \textbf{Modular integration:} RAAH predictions can be integrated via prompt engineering, template-based rephrasing, or LLM-based caption refinement.
\end{enumerate}

\subsubsection{Dataset Curation}

RAAH requires a curated dataset of region-level captions with explicit attribute annotations. We construct this dataset by:

\paragraph{Step 1: Region Selection.} From COCO and Visual Genome datasets, we identify regions containing small or medium-sized objects (bounding box area $< 0.15 \times$ image area) that are likely to have distinctive visual attributes.

\paragraph{Step 2: Caption Collection.} For each region, we collect reference captions from existing annotations (e.g., Visual Genome region descriptions).

\paragraph{Step 3: Attribute Extraction.} We apply rule-based and LLM-assisted parsers to extract attribute mentions from captions. Specifically, for each caption, we identify:

\begin{itemize}
    \item \textbf{Color attributes:} Adjectives describing color (e.g., "red", "blue", "metallic")
    \item \textbf{Material attributes:} Nouns/adjectives describing material (e.g., "wooden", "metal", "glass")
    \item \textbf{Shape attributes:} Adjectives describing shape (e.g., "round", "rectangular", "cylindrical")
    \item \textbf{Texture attributes:} Adjectives describing surface texture (e.g., "smooth", "rough", "striped")
\end{itemize}

Each sample in the curated dataset $\mathcal{D}$ consists of:

\begin{equation}
\mathcal{D} = \{(\mathcal{I}_i, \mathcal{R}_i, c_i, \mathcal{A}_i)\}_{i=1}^{|\mathcal{D}|}
\end{equation}

where $\mathcal{I}_i$ is the image, $\mathcal{R}_i$ is the region, $c_i$ is the caption, and $\mathcal{A}_i = \{(a_j^{\text{type}}, a_j^{\text{value}})\}$ is a set of attribute annotations.

\paragraph{Attribute Vocabulary Construction.} We build a vocabulary for each attribute type by collecting all unique attribute values and filtering by minimum frequency (threshold $f_{\min} = 2$):

\begin{equation}
\mathcal{V}_{\text{attr}}^{(k)} = \{v \mid \text{count}(v, \text{type}=k) \geq f_{\min}\}
\end{equation}

where $k \in \{\text{color}, \text{material}, \text{shape}, \text{texture}\}$. Vocabulary sizes are:

\begin{equation}
|\mathcal{V}_{\text{color}}| = 19, \quad |\mathcal{V}_{\text{material}}| = 12, \quad |\mathcal{V}_{\text{shape}}| = 10, \quad |\mathcal{V}_{\text{texture}}| = 11
\end{equation}

\subsubsection{Hidden State Extraction from DAM}

For each sample in $\mathcal{D}$, we run DAM inference to generate a caption and extract decoder hidden states. Specifically, during autoregressive generation, we collect the final layer's hidden states at each timestep:

\begin{equation}
\mathbf{H}_{\text{DAM}} = [\mathbf{h}_{L,1}, \mathbf{h}_{L,2}, \ldots, \mathbf{h}_{L,T}] \in \mathbb{R}^{T \times d_h}
\end{equation}

where $T$ is the generated caption length, $L = 32$ is the final transformer layer, and $d_h = 2560$ (or $768$ for smaller DAM variants).

These hidden states encode:
\begin{itemize}
    \item Visual information from the region (via cross-attention to visual tokens $\mathbf{X}_v$)
    \item Linguistic context from previously generated tokens
    \item Semantic representations learned during DAM's pre-training
\end{itemize}

Crucially, $\mathbf{H}_{\text{DAM}}$ is extracted with \textbf{DAM frozen}, enabling efficient dataset creation without gradient computation.

\subsubsection{RAAH Architecture}

RAAH consists of two components: (1) a region pooling module that aggregates variable-length hidden states into a fixed-size representation, and (2) a multi-head attribute classifier that predicts visual attributes.

\paragraph{Region Pooling Module.} To handle variable-length caption sequences $\mathbf{H}_{\text{DAM}} \in \mathbb{R}^{T \times d_h}$ where $T$ varies across samples, we apply a pooling operation $\mathcal{F}_{\text{pool}}$ to produce a fixed-size region-level representation:

\begin{equation}
\mathbf{r} = \mathcal{F}_{\text{pool}}(\mathbf{H}_{\text{DAM}}, \mathbf{m}) \in \mathbb{R}^{d_h}
\end{equation}

where $\mathbf{m} \in \{0,1\}^T$ is an attention mask indicating valid tokens (1) vs. padding (0).

We implement four pooling strategies:

\begin{enumerate}
    \item \textbf{Mean pooling:}
    \begin{equation}
    \mathbf{r}_{\text{mean}} = \frac{1}{\sum_{t=1}^T m_t} \sum_{t=1}^T m_t \mathbf{h}_{L,t}
    \end{equation}

    \item \textbf{Max pooling:}
    \begin{equation}
    \mathbf{r}_{\text{max}} = \max_{t=1}^T (m_t \mathbf{h}_{L,t})
    \end{equation}
    where masked positions are set to $-\infty$ before max operation.

    \item \textbf{Last-token pooling:}
    \begin{equation}
    \mathbf{r}_{\text{last}} = \mathbf{h}_{L, T'}
    \end{equation}
    where $T' = \sum_{t=1}^T m_t$ is the index of the last valid token.

    \item \textbf{Attention-based pooling:} Learn a scoring function $\mathbf{w} \in \mathbb{R}^{d_h}$:
    \begin{align}
    \alpha_t &= \frac{\exp(\mathbf{w}^T \mathbf{h}_{L,t})}{\sum_{t'=1}^T m_{t'} \exp(\mathbf{w}^T \mathbf{h}_{L,t'})} \\
    \mathbf{r}_{\text{attn}} &= \sum_{t=1}^T \alpha_t m_t \mathbf{h}_{L,t}
    \end{align}
\end{enumerate}

In our experiments, we find \textbf{mean pooling} provides the best balance between performance and simplicity.

\paragraph{Multi-Head Attribute Classifier.} The pooled representation $\mathbf{r}$ is passed through a shared feature transformation followed by attribute-specific classification heads:

\begin{align}
\mathbf{f}_{\text{shared}} &= \text{Dropout}\left(\text{ReLU}\left(\text{LayerNorm}(\mathbf{W}_{\text{shared}} \mathbf{r} + \mathbf{b}_{\text{shared}})\right)\right) \\
\mathbf{z}_k &= \mathbf{W}_k \mathbf{f}_{\text{shared}} + \mathbf{b}_k, \quad k \in \{\text{color}, \text{material}, \text{shape}, \text{texture}\}
\end{align}

where:
\begin{itemize}
    \item $\mathbf{W}_{\text{shared}} \in \mathbb{R}^{d_{\text{hidden}} \times d_h}$ projects from DAM hidden dimension to RAAH hidden dimension ($d_{\text{hidden}} = 512$)
    \item LayerNorm normalizes activations for stable training
    \item ReLU introduces non-linearity
    \item Dropout (rate $p = 0.1$) prevents overfitting
    \item $\mathbf{W}_k \in \mathbb{R}^{|\mathcal{V}_{\text{attr}}^{(k)}| \times d_{\text{hidden}}}$ is the classification head for attribute type $k$
    \item $\mathbf{z}_k \in \mathbb{R}^{|\mathcal{V}_{\text{attr}}^{(k)}|}$ are logits for attribute type $k$
\end{itemize}

The complete RAAH forward pass is:

\begin{equation}
\boxed{
\begin{aligned}
\mathbf{r} &= \mathcal{F}_{\text{pool}}(\mathbf{H}_{\text{DAM}}, \mathbf{m}) \\
\mathbf{f} &= \text{Dropout}(\text{ReLU}(\text{LayerNorm}(\mathbf{W}_{\text{shared}} \mathbf{r} + \mathbf{b}_{\text{shared}}))) \\
\mathbf{z}_k &= \mathbf{W}_k \mathbf{f} + \mathbf{b}_k, \quad \forall k \in \mathcal{K}
\end{aligned}
}
\end{equation}

where $\mathcal{K} = \{\text{color}, \text{material}, \text{shape}, \text{texture}\}$ is the set of attribute types.

\subsubsection{Training Objective}

RAAH is trained as a multi-label classification task, where each sample can have multiple attributes per type (e.g., "red and blue striped"). We encode ground-truth attributes as multi-hot vectors:

\begin{equation}
\mathbf{y}_k \in \{0,1\}^{|\mathcal{V}_{\text{attr}}^{(k)}|}, \quad (\mathbf{y}_k)_j =
\begin{cases}
1 & \text{if attribute } j \text{ is present for type } k \\
0 & \text{otherwise}
\end{cases}
\end{equation}

The loss for attribute type $k$ is binary cross-entropy with logits:

\begin{equation}
\mathcal{L}_k = -\frac{1}{|\mathcal{V}_{\text{attr}}^{(k)}|} \sum_{j=1}^{|\mathcal{V}_{\text{attr}}^{(k)}|} \left[ y_{k,j} \log \sigma(z_{k,j}) + (1 - y_{k,j}) \log(1 - \sigma(z_{k,j})) \right]
\end{equation}

where $\sigma(z) = 1/(1 + e^{-z})$ is the sigmoid function. The total loss averages across all attribute types:

\begin{equation}
\boxed{
\mathcal{L}_{\text{RAAH}} = \frac{1}{|\mathcal{K}|} \sum_{k \in \mathcal{K}} \mathcal{L}_k
}
\end{equation}

\paragraph{Handling Class Imbalance.} To address the long-tail distribution of attributes (e.g., "white" is much more frequent than "magenta"), we optionally apply positive class weighting:

\begin{equation}
\mathcal{L}_k^{\text{weighted}} = -\frac{1}{|\mathcal{V}_{\text{attr}}^{(k)}|} \sum_{j=1}^{|\mathcal{V}_{\text{attr}}^{(k)}|} \left[ w_{k,j} y_{k,j} \log \sigma(z_{k,j}) + (1 - y_{k,j}) \log(1 - \sigma(z_{k,j})) \right]
\end{equation}

where $w_{k,j} = \frac{N_{\text{neg}}}{N_{\text{pos}}}$ is the ratio of negative to positive samples for attribute $(k,j)$.

\subsubsection{Training Procedure}

RAAH is trained using the following procedure:

\begin{algorithm}[H]
\caption{RAAH Training}
\begin{algorithmic}[1]
\REQUIRE Curated dataset $\mathcal{D} = \{(\mathcal{I}_i, \mathcal{R}_i, c_i, \mathcal{A}_i)\}$, frozen DAM model, vocabulary $\{\mathcal{V}_{\text{attr}}^{(k)}\}$
\ENSURE Trained RAAH classifier

\STATE \textbf{Step 1: Extract DAM hidden states}
\FOR{each sample $(\mathcal{I}_i, \mathcal{R}_i, c_i, \mathcal{A}_i) \in \mathcal{D}$}
    \STATE Run DAM inference: $\hat{c}_i, \mathbf{H}_{\text{DAM}}^{(i)} \gets \text{DAM}(\mathcal{I}_i, \mathcal{R}_i)$
    \STATE Save hidden states: $\text{save}(\mathbf{H}_{\text{DAM}}^{(i)}, \text{path}_i)$
\ENDFOR

\STATE \textbf{Step 2: Prepare training data}
\STATE Split $\mathcal{D}$ into $\mathcal{D}_{\text{train}}$ (80\%) and $\mathcal{D}_{\text{val}}$ (20\%)
\STATE Encode attributes $\mathcal{A}_i$ into multi-hot vectors $\{\mathbf{y}_k^{(i)}\}$

\STATE \textbf{Step 3: Train RAAH}
\STATE Initialize RAAH parameters $\theta = \{\mathbf{W}_{\text{shared}}, \mathbf{b}_{\text{shared}}, \{\mathbf{W}_k, \mathbf{b}_k\}_{k \in \mathcal{K}}\}$
\STATE Initialize optimizer: $\text{opt} \gets \text{AdamW}(\theta, \text{lr}=10^{-3}, \text{weight\_decay}=10^{-4})$
\STATE Initialize scheduler: $\text{sched} \gets \text{CosineAnnealingLR}(\text{opt}, T_{\max}=20)$

\FOR{epoch $= 1$ to $20$}
    \FOR{each batch $\{(\mathbf{H}_{\text{DAM}}^{(i)}, \{\mathbf{y}_k^{(i)}\})\} \in \mathcal{D}_{\text{train}}$}
        \STATE Forward pass: $\{\mathbf{z}_k\} \gets \text{RAAH}(\mathbf{H}_{\text{DAM}})$
        \STATE Compute loss: $\mathcal{L} \gets \frac{1}{|\mathcal{K}|} \sum_{k \in \mathcal{K}} \mathcal{L}_k(\mathbf{z}_k, \mathbf{y}_k)$
        \STATE Backward pass and update: $\theta \gets \text{opt.step}(\nabla_\theta \mathcal{L})$
    \ENDFOR
    \STATE Evaluate on $\mathcal{D}_{\text{val}}$ and save best checkpoint
    \STATE $\text{sched.step}()$
\ENDFOR

\RETURN Best RAAH checkpoint
\end{algorithmic}
\end{algorithm}

\paragraph{Hyperparameters.} We use the following configuration for all experiments:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
DAM hidden dimension $d_h$ & 768 (DAM-2B) / 2560 (DAM-7B) \\
RAAH hidden dimension $d_{\text{hidden}}$ & 512 \\
Pooling type & Mean \\
Dropout rate $p$ & 0.1 \\
Batch size & 32 \\
Learning rate & $10^{-3}$ \\
Weight decay & $10^{-4}$ \\
Optimizer & AdamW \\
Scheduler & CosineAnnealingLR \\
Number of epochs & 20 \\
Gradient clipping & None \\
\bottomrule
\end{tabular}
\caption{RAAH training hyperparameters.}
\label{tab:hyperparams}
\end{table}

\subsubsection{Inference and Attribute Prediction}

At inference time, RAAH predicts attributes from DAM hidden states as follows:

\begin{algorithm}[H]
\caption{RAAH Inference}
\begin{algorithmic}[1]
\REQUIRE Image $\mathcal{I}$, region $\mathcal{R}$, trained RAAH model, threshold $\tau = 0.5$, top-$K = 3$
\ENSURE Predicted attributes $\hat{\mathcal{A}}$

\STATE \textbf{Step 1: Generate caption with DAM}
\STATE $\hat{c}, \mathbf{H}_{\text{DAM}} \gets \text{DAM}(\mathcal{I}, \mathcal{R})$

\STATE \textbf{Step 2: Predict attributes with RAAH}
\STATE $\{\mathbf{z}_k\} \gets \text{RAAH}(\mathbf{H}_{\text{DAM}})$
\STATE $\hat{\mathcal{A}} \gets \emptyset$
\FOR{each attribute type $k \in \mathcal{K}$}
    \STATE Compute probabilities: $\mathbf{p}_k = \sigma(\mathbf{z}_k)$
    \STATE Get top-$K$ predictions: $(p_{k,j_1}, \ldots, p_{k,j_K}) \gets \text{topk}(\mathbf{p}_k, K)$
    \FOR{$i = 1$ to $K$}
        \IF{$p_{k,j_i} \geq \tau$}
            \STATE $\hat{\mathcal{A}} \gets \hat{\mathcal{A}} \cup \{(k, v_{k,j_i}, p_{k,j_i})\}$ \COMMENT{Add attribute}
        \ENDIF
    \ENDFOR
\ENDFOR

\RETURN Predicted attributes $\hat{\mathcal{A}} = \{(k, v, p)\}$
\end{algorithmic}
\end{algorithm}

where each predicted attribute is a tuple $(k, v, p)$ containing the attribute type, value, and confidence score.

\subsubsection{Caption Augmentation Strategy}

RAAH's predicted attributes $\hat{\mathcal{A}}$ are integrated with DAM's caption $\hat{c}$ to produce an augmented caption $\hat{c}_{\text{aug}}$. We implement two augmentation strategies:

\paragraph{Strategy 1: Template-Based Insertion.} We identify the main noun phrase in $\hat{c}$ (e.g., "a car") and prepend predicted attributes:

\begin{equation}
\hat{c}_{\text{aug}} = \text{insert}(\hat{c}, \text{noun\_phrase}, \{\text{attr\_values from } \hat{\mathcal{A}}\})
\end{equation}

Example:
\begin{align*}
\hat{c} &= \text{``A car parked on the street''} \\
\hat{\mathcal{A}} &= \{(\text{color}, \text{red}, 0.92), (\text{material}, \text{metal}, 0.78)\} \\
\hat{c}_{\text{aug}} &= \text{``A red metal car parked on the street''}
\end{align*}

\paragraph{Strategy 2: LLM-Based Rephrasing.} We use a language model (GPT-4 or Gemma-7B) with a structured prompt:

\begin{verbatim}
Given:
- Original caption: "{hat_c}"
- Predicted attributes: "{hat_A}"

Task: Rewrite the caption to naturally incorporate the predicted
attributes without changing the overall meaning. Keep the caption
concise and grammatically correct.

Augmented caption:
\end{verbatim}

This approach produces more natural language integration but requires an additional LLM call.

\subsubsection{Evaluation Metrics}

We evaluate RAAH on two axes:

\paragraph{Attribute Prediction Accuracy.} On held-out test data with ground-truth attributes, we compute:

\begin{align}
\text{Precision}_k &= \frac{\text{TP}_k}{\text{TP}_k + \text{FP}_k}, \quad
\text{Recall}_k = \frac{\text{TP}_k}{\text{TP}_k + \text{FN}_k} \\
\text{F1}_k &= \frac{2 \cdot \text{Precision}_k \cdot \text{Recall}_k}{\text{Precision}_k + \text{Recall}_k}
\end{align}

for each attribute type $k$, and aggregate overall F1:

\begin{equation}
\text{F1}_{\text{overall}} = \frac{2 \cdot \text{Precision}_{\text{overall}} \cdot \text{Recall}_{\text{overall}}}{\text{Precision}_{\text{overall}} + \text{Recall}_{\text{overall}}}
\end{equation}

where $\text{TP}_{\text{overall}} = \sum_k \text{TP}_k$, etc.

\paragraph{Caption Quality Metrics.} We evaluate augmented captions on DLC-Bench using:

\begin{itemize}
    \item \textbf{CIDEr:} Consensus-based metric measuring n-gram overlap with reference captions
    \item \textbf{SPICE:} Semantic metric based on scene graph matching
    \item \textbf{BLEU-4:} N-gram precision up to 4-grams
    \item \textbf{LLM-Judge:} GPT-4-based evaluation of caption quality and attribute accuracy
\end{itemize}

\subsection{Comparison: Baseline vs. RAAH}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Aspect} & \textbf{DAM-3B Baseline} & \textbf{DAM-3B + RAAH} \\
\midrule
Architecture modification & None & Lightweight MLP head (650K params) \\
Training required & Pre-trained DAM & RAAH only (DAM frozen) \\
Inference components & DAM & DAM + RAAH + LLM rephraser (optional) \\
Attribute extraction & Implicit (from captions) & Explicit (multi-label classification) \\
Caption augmentation & N/A & Template or LLM-based \\
Inference overhead & Baseline & +5-10\% (RAAH forward pass) \\
Handles small objects & Moderate & Improved (via attribute emphasis) \\
Hallucination mitigation & Limited & Improved (grounded attributes) \\
\bottomrule
\end{tabular}
\caption{Comparison of baseline DAM and RAAH-enhanced pipeline.}
\label{tab:comparison}
\end{table}

\subsection{Novelty and Contributions}

Our approach makes four key contributions:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Training-free DAM enhancement:} RAAH operates entirely on frozen DAM hidden states, enabling rapid experimentation without expensive re-training of large vision-language models.

    \item \textbf{Fine-grained attribute recovery:} Unlike generic caption metrics (CIDEr, SPICE) that measure overall semantic similarity, RAAH explicitly extracts and evaluates visual attributes (color, material, shape, texture) that are critical for detailed region descriptions.

    \item \textbf{Modular and interpretable:} RAAH's multi-head architecture provides interpretable attribute predictions with confidence scores, enabling selective caption augmentation and error analysis.

    \item \textbf{Curated small-object dataset:} We introduce a dataset curation pipeline that identifies challenging small-object regions and extracts structured attribute annotations, facilitating future research on attribute-aware region captioning.
\end{enumerate}

\subsection{Expected Outcomes}

We hypothesize that RAAH will:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Improve attribute recall:} Increase the presence of correct visual attributes in generated captions, measured via attribute F1 score on annotated test data.

    \item \textbf{Reduce hallucinations:} Decrease the frequency of incorrect attribute mentions (e.g., claiming an object is "red" when it is actually "blue") by grounding predictions in learned attribute classifiers.

    \item \textbf{Maintain or improve caption quality:} Preserve high CIDEr/SPICE scores while adding fine-grained details, validated via LLM-judge evaluations.

    \item \textbf{Generalize to new domains:} Transfer to other vision-language models (e.g., LLaVA, Qwen-VL) by retraining RAAH on their hidden states without modifying base model architectures.
\end{enumerate}

\section{Reproducibility}

All architectural details, equations, and hyperparameters have been specified to enable full replication. The implementation uses:

\begin{itemize}
    \item \textbf{Base model:} DAM-3B or DAM-2B from NVlabs/describe-anything
    \item \textbf{Framework:} PyTorch 2.0+ with standard libraries (no custom CUDA kernels)
    \item \textbf{Dataset:} Curated from COCO + Visual Genome with attribute annotations
    \item \textbf{RAAH architecture:}
    \begin{itemize}
        \item Region pooling: Mean pooling over decoder hidden states
        \item Shared transform: Linear($d_h \to 512$) + LayerNorm + ReLU + Dropout(0.1)
        \item Classification heads: 4 linear layers (one per attribute type)
    \end{itemize}
    \item \textbf{Training:} AdamW optimizer, CosineAnnealingLR scheduler, 20 epochs, batch size 32
\end{itemize}

Code, trained models, and curated dataset will be released to facilitate reproduction and extension.

\end{document}
